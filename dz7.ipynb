{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "#from tensorflow.keras.layers import BatchNormalization\n",
    "#from tensorflow.keras.layers import Layer, InputSpec\n",
    "from imageai.Detection import ObjectDetection\n",
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "from imageai.Detection.Custom import CustomObjectDetection\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - 240, test -60\n"
     ]
    }
   ],
   "source": [
    "train_path = 'data_dz7/train_zip/train/'\n",
    "test_path = 'data_dz7/test_zip/test/'\n",
    "\n",
    "annots_train_path = sorted([i for i in Path(train_path).glob('*.xml')])\n",
    "images_train_path = sorted([i for i in Path(train_path).glob('*.jpg')])\n",
    "\n",
    "annots_test_path = sorted([i for i in Path(test_path).glob('*.xml')])\n",
    "images_test_path = sorted([i for i in Path(test_path).glob('*.xml')])\n",
    "\n",
    "classes = np.array([\"Apple\", 'Banana', 'Orange'])\n",
    "\n",
    "n_imgs = len(images_train_path)\n",
    "print(f'train - {n_imgs}, test -{len(images_test_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editXML(file):\n",
    "    \n",
    "    my_file = open(file, 'r')\n",
    "    string = my_file.read()\n",
    "    \n",
    "    width = string[string.find('<width>')+len('<width>') : string.find('</width>')]\n",
    "    height = string[string.find('<height>')+len('<height>') : string.find('</height>')]\n",
    "    \n",
    "    if width == '0' or height == '0':\n",
    "        \n",
    "        folder = string[string.find('<folder>')+len('<folder>') : string.find('</folder>')]\n",
    "        filename = string[string.find('<filename>')+len('<filename>') : string.find('</filename>')]\n",
    "        path_img = f'data_dz7/{folder}_zip/{folder}/{filename}'\n",
    "        \n",
    "        im = Image.open(path_img)\n",
    "        (width_im, height_im) = im.size\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for elem in root.iter(\"width\"):\n",
    "            elem.text = str(width_im)\n",
    "\n",
    "        for elem in root.iter(\"height\"):\n",
    "            elem.text = str(height_im)\n",
    "\n",
    "        tree.write('newfile.xml')\n",
    "\n",
    "        return 'newfile.xml'\n",
    "    \n",
    "    else:\n",
    "        return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_image_and_XML(file):\n",
    "\n",
    "    my_file = open(file, 'r')\n",
    "    string = my_file.read()\n",
    "\n",
    "    path_image = file.replace('annotations', 'images')\n",
    "    image = cv2.imread(path_image[:-3]+'jpg')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    bboxes = []\n",
    "    class_labels = []\n",
    "    \n",
    "    for item in string.split('</object>'):\n",
    "        if '<object>' in item:\n",
    "            bboxes.append([int(item[item.find('<xmin>')+len('<xmin>') : item.find('</xmin>')]),\n",
    "                           int(item[item.find('<ymin>')+len('<ymin>') : item.find('</ymin>')]),\n",
    "                           int(item[item.find('<xmax>')+len('<xmax>') : item.find('</xmax>')]),\n",
    "                           int(item[item.find('<ymax>')+len('<ymax>') : item.find('</ymax>')])])\n",
    "            class_labels.append(item[item.find('<name>')+len('<name>') : item.find('</name>')])\n",
    "\n",
    "    transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ChannelDropout(channel_drop_range=(1, 1), fill_value=0, p=0.1),\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', min_area=1024, min_visibility=0.1, label_fields=['class_labels']))\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "        transformed_image = transformed['image']\n",
    "        transformed_bboxes = transformed['bboxes']\n",
    "        transformed_class_labels = transformed['class_labels']\n",
    "\n",
    "        # сохранить изображение\n",
    "        Image.fromarray(transformed_image).save(f'{path_image[:-4]}-{i}.jpg',quality=95)\n",
    "        \n",
    "        # сохранить аннотацию \n",
    "        folder = string[string.find('<folder>')+len('<folder>') : string.find('</folder>')]\n",
    "        filename = string[string.find('<filename>')+len('<filename>') : string.find('</filename>')]\n",
    "        width = string[string.find('<width>')+len('<width>') : string.find('</width>')]\n",
    "        height = string[string.find('<height>')+len('<height>') : string.find('</height>')]\n",
    "        \n",
    "        annotation = ET.Element(\"annotation\")\n",
    "        ET.SubElement(annotation, \"folder\").text = folder\n",
    "        ET.SubElement(annotation, \"filename\").text = str(f'{filename[:-4]}-{i}.jpg')\n",
    "        ET.SubElement(annotation, \"path\").text = str(f'{path_image[:-4]}-{i}.jpg')\n",
    "        source = ET.SubElement(annotation, \"source\")\n",
    "        ET.SubElement(source, \"database\").text = \"Unknown\"\n",
    "        size = ET.SubElement(annotation, \"size\")\n",
    "        ET.SubElement(size, \"width\").text = str(width)\n",
    "        ET.SubElement(size, \"height\").text = str(height)\n",
    "        ET.SubElement(size, \"depth\").text = \"3\"\n",
    "        ET.SubElement(annotation, \"segmented\").text = \"0\"\n",
    "        for num, box in enumerate(transformed_bboxes):\n",
    "            object = ET.SubElement(annotation, \"object\")\n",
    "            ET.SubElement(object, \"name\").text = transformed_class_labels[num]\n",
    "            ET.SubElement(object, \"pose\").text = \"Unspecified\"\n",
    "            ET.SubElement(object, \"truncated\").text = \"0\"\n",
    "            ET.SubElement(object, \"difficult\").text = \"0\"\n",
    "            bndbox = ET.SubElement(object, \"bndbox\")\n",
    "            ET.SubElement(bndbox, \"xmin\").text = str(int(box[0])) \n",
    "            ET.SubElement(bndbox, \"ymin\").text = str(int(box[1]))\n",
    "            ET.SubElement(bndbox, \"xmax\").text = str(int(box[2]))\n",
    "            ET.SubElement(bndbox, \"ymax\").text = str(int(box[3]))\n",
    "\n",
    "        tree = ET.ElementTree(annotation)\n",
    "        tree.write(f'{file[:-4]}-{i}.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание директорий для сохранение предобработанных данных\n",
    "os.makedirs('imageai/data/train/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/train/annotations', exist_ok=True)\n",
    "\n",
    "os.makedirs('imageai/data/validation/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/validation/annotations', exist_ok=True)\n",
    "\n",
    "os.makedirs('imageai/data/test/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/test/annotations', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple' 'orange' 'banana']\n"
     ]
    }
   ],
   "source": [
    "# формирование списка классов\n",
    "set_classes = set()\n",
    "annots_path = annots_train_path + annots_test_path\n",
    "\n",
    "for annot in annots_path:\n",
    "    with open(annot, 'r') as f:\n",
    "        string = f.read()\n",
    "        name = string[string.find('<name>')+6:string.find('</name>')]\n",
    "        set_classes.add(name)\n",
    "\n",
    "classes = np.array(list(set_classes))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (annot_path, img_path) in enumerate(zip(annots_train_path, images_train_path)):   \n",
    "    if i > n_imgs:\n",
    "        break\n",
    "        \n",
    "    if i%5 == 0:\n",
    "        new_path = shutil.copy(img_path, 'imageai/data/validation/images/' + img_path.parts[-1])\n",
    "        # редактирование валидационных данных\n",
    "        annot_path_edit = editXML(annot_path)\n",
    "        annot_path_new = shutil.copy(annot_path_edit, 'imageai/data/validation/annotations/' + annot_path.parts[-1])\n",
    "        # аугментация валидационных данных\n",
    "        augmentation_image_and_XML(annot_path_new)\n",
    "    else:\n",
    "        shutil.copy(img_path, 'imageai/data/train/images/' + img_path.parts[-1])\n",
    "        # редактирование тренировочных данных\n",
    "        annot_path_edit = editXML(annot_path)\n",
    "        annot_path_new = shutil.copy(annot_path_edit, 'imageai/data/train/annotations/' + annot_path.parts[-1])\n",
    "        # аугментация тренировочных данных\n",
    "        augmentation_image_and_XML(annot_path_new)\n",
    "        \n",
    "for i, (annot_test_path, img_test_path) in enumerate(zip(annots_test_path, images_test_path)):  \n",
    "    shutil.copy(img_test_path, 'imageai/data/test/images/' + img_test_path.parts[-1])\n",
    "    annot_test_path_new = editXML(annot_test_path)\n",
    "    shutil.copy(annot_test_path_new, 'imageai/data/test/annotations/' + annot_test_path.parts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"data_dz7/yolo.h5\")\n",
    "detector.loadModel()\n",
    "detections = detector.detectObjectsFromImage(input_image='data_dz7/train_zip/train/apple_31.jpg',\n",
    "                                             output_image_path='detected.jpg',\n",
    "                                             minimum_percentage_probability=50)\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split = n_imgs // 6\n",
    "\n",
    "\n",
    "for i, (annot_path, img_path) in enumerate(zip(annots_train_path, images_train_path)):   \n",
    "    if i > n_imgs:\n",
    "        break\n",
    "        \n",
    "    if i%5 == 0:\n",
    "        new_path = shutil.copy(img_path, 'imageai/data/validation/images/' + img_path.parts[-1])\n",
    "        # редактирование валидационных данных\n",
    "        annot_path_edit = editXML(annot_path)\n",
    "        annot_path_new = shutil.copy(annot_path_edit, 'imageai/data/validation/annotations/' + annot_path.parts[-1])\n",
    "        # аугментация валидационных данных\n",
    "        augmentation_image_and_XML(annot_path_new)\n",
    "    else:\n",
    "        shutil.copy(img_path, 'imageai/data/train/images/' + img_path.parts[-1])\n",
    "        # редактирование тренировочных данных\n",
    "        annot_path_edit = editXML(annot_path)\n",
    "        annot_path_new = shutil.copy(annot_path_edit, 'imageai/data/train/annotations/' + annot_path.parts[-1])\n",
    "        # аугментация тренировочных данных\n",
    "        augmentation_image_and_XML(annot_path_new)\n",
    "        \n",
    "for i, (annot_test_path, img_test_path) in enumerate(zip(annots_test_path, images_test_path)):  \n",
    "    shutil.copy(img_test_path, 'imageai/data/test/images/' + img_test_path.parts[-1])\n",
    "    annot_test_path_new = editXML(annot_test_path)\n",
    "    shutil.copy(annot_test_path_new, 'imageai/data/test/annotations/' + annot_test_path.parts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor boxes for training images and annotation...\n",
      "Average IOU for 9 anchors: 0.81\n",
      "Anchor Boxes generated.\n",
      "Detection configuration saved in  ./imageai/data/json\\detection_config.json\n",
      "Training on: \t['apple', 'banana', 'orange']\n",
      "Training with Batch Size:  8\n",
      "Number of Experiments:  30\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"yolo_layer_1\" (type YoloLayer).\n\nin user code:\n\n    File \"D:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\Custom\\yolo.py\", line 149, in call  *\n        batch_seen = tf.assign_add(batch_seen, 1.)\n\n    AttributeError: module 'tensorflow' has no attribute 'assign_add'\n\n\nCall arguments received:\n  • x=['tf.Tensor(shape=(None, None, None, 3), dtype=float32)', 'tf.Tensor(shape=(None, None, None, 24), dtype=float32)', 'tf.Tensor(shape=(None, None, None, 3, 8), dtype=float32)', 'tf.Tensor(shape=(None, 1, 1, 1, 9, 4), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39msetTrainConfig(object_names_array\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m      9\u001b[0m                        batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     10\u001b[0m                        num_experiments\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     11\u001b[0m                        train_from_pretrained_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/gb-pretrainedyolov3h5/pretrained-yolov3.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#                        train_from_pretrained_model=\"imageai/data/models/detection_model-ex-020--loss-0036.609.h5\")\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\Custom\\__init__.py:260\u001b[0m, in \u001b[0;36mDetectionModelTrainer.trainModel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__train_gpus\n\u001b[0;32m    258\u001b[0m multi_gpu \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(gpu) \u001b[38;5;28;01mfor\u001b[39;00m gpu \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__train_gpus\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m--> 260\u001b[0m train_model, infer_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnb_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__model_anchors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_box_per_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_box_per_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__model_max_input_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__model_max_input_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_ignore_treshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_learning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_grid_scales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_obj_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoobj_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_noobj_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxywh_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_xywh_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_class_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m###############################\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m#   Kick off the training\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m###############################\u001b[39;00m\n\u001b[0;32m    280\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__train_weights_name, infer_model)\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\Custom\\__init__.py:541\u001b[0m, in \u001b[0;36mDetectionModelTrainer._create_model\u001b[1;34m(self, nb_class, anchors, max_box_per_image, max_grid, batch_size, warmup_batches, ignore_thresh, multi_gpu, lr, grid_scales, obj_scale, noobj_scale, xywh_scale, class_scale)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(multi_gpu) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cpu:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 541\u001b[0m         template_model, infer_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_yolov3_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnb_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m            \u001b[49m\u001b[43manchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_box_per_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_box_per_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmulti_gpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwarmup_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrid_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_scales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnoobj_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoobj_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m            \u001b[49m\u001b[43mxywh_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxywh_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_scale\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    556\u001b[0m     template_model, infer_model \u001b[38;5;241m=\u001b[39m create_yolov3_model(\n\u001b[0;32m    557\u001b[0m         nb_class\u001b[38;5;241m=\u001b[39mnb_class,\n\u001b[0;32m    558\u001b[0m         anchors\u001b[38;5;241m=\u001b[39manchors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    568\u001b[0m         class_scale\u001b[38;5;241m=\u001b[39mclass_scale\n\u001b[0;32m    569\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\Custom\\yolo.py:285\u001b[0m, in \u001b[0;36mcreate_yolov3_model\u001b[1;34m(nb_class, anchors, max_box_per_image, max_grid, batch_size, warmup_batches, ignore_thresh, grid_scales, obj_scale, noobj_scale, xywh_scale, class_scale)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Layer 80 => 82\u001b[39;00m\n\u001b[0;32m    283\u001b[0m pred_yolo_1 \u001b[38;5;241m=\u001b[39m _conv_block(x, [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbnorm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleaky\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m80\u001b[39m},\n\u001b[0;32m    284\u001b[0m                          {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m+\u001b[39mnb_class)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbnorm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleaky\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m81\u001b[39m}], do_skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 285\u001b[0m loss_yolo_1 \u001b[38;5;241m=\u001b[39m \u001b[43mYoloLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_grid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwarmup_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mignore_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgrid_scales\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mobj_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnoobj_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mxywh_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mclass_scale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_yolo_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_yolo_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_boxes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# Layer 83 => 86\u001b[39;00m\n\u001b[0;32m    297\u001b[0m x \u001b[38;5;241m=\u001b[39m _conv_block(x, [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m256\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbnorm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleaky\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m84\u001b[39m}], do_skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Exception encountered when calling layer \"yolo_layer_1\" (type YoloLayer).\n\nin user code:\n\n    File \"D:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\Custom\\yolo.py\", line 149, in call  *\n        batch_seen = tf.assign_add(batch_seen, 1.)\n\n    AttributeError: module 'tensorflow' has no attribute 'assign_add'\n\n\nCall arguments received:\n  • x=['tf.Tensor(shape=(None, None, None, 3), dtype=float32)', 'tf.Tensor(shape=(None, None, None, 24), dtype=float32)', 'tf.Tensor(shape=(None, None, None, 3, 8), dtype=float32)', 'tf.Tensor(shape=(None, 1, 1, 1, 9, 4), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=\"imageai/data/\")\n",
    "metrics = trainer.evaluateModel(model_path=model_path,\n",
    "                                json_path=\"imageai/data/json/detection_config.json\",\n",
    "                                iou_threshold=0.2,\n",
    "                                object_threshold=0.3,\n",
    "                                nms_threshold=0.5)\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
