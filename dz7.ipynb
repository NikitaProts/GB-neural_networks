{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "#from tensorflow.keras.layers import BatchNormalization\n",
    "#from tensorflow.keras.layers import Layer, InputSpec\n",
    "from imageai.Detection import ObjectDetection\n",
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "from imageai.Detection.Custom import CustomObjectDetection\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - 240, test -60\n"
     ]
    }
   ],
   "source": [
    "train_path = 'data_dz7/train_zip/train/'\n",
    "test_path = 'data_dz7/test_zip/test/'\n",
    "\n",
    "annots_train_path = sorted([i for i in Path(train_path).glob('*.xml')])\n",
    "images_train_path = sorted([i for i in Path(train_path).glob('*.jpg')])\n",
    "\n",
    "annots_test_path = sorted([i for i in Path(test_path).glob('*.xml')])\n",
    "images_test_path = sorted([i for i in Path(test_path).glob('*.xml')])\n",
    "\n",
    "classes = np.array([\"Apple\", 'Banana', 'Orange'])\n",
    "\n",
    "n_imgs = len(images_train_path)\n",
    "print(f'train - {n_imgs}, test -{len(images_test_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editXML(file):\n",
    "    \n",
    "    my_file = open(file, 'r')\n",
    "    string = my_file.read()\n",
    "    \n",
    "    width = string[string.find('<width>')+len('<width>') : string.find('</width>')]\n",
    "    height = string[string.find('<height>')+len('<height>') : string.find('</height>')]\n",
    "    \n",
    "    if width == '0' or height == '0':\n",
    "        \n",
    "        folder = string[string.find('<folder>')+len('<folder>') : string.find('</folder>')]\n",
    "        filename = string[string.find('<filename>')+len('<filename>') : string.find('</filename>')]\n",
    "        path_img = f'data_dz7/{folder}_zip/{folder}/{filename}'\n",
    "        \n",
    "        im = Image.open(path_img)\n",
    "        (width_im, height_im) = im.size\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for elem in root.iter(\"width\"):\n",
    "            elem.text = str(width_im)\n",
    "\n",
    "        for elem in root.iter(\"height\"):\n",
    "            elem.text = str(height_im)\n",
    "\n",
    "        tree.write('newfile.xml')\n",
    "\n",
    "        return 'newfile.xml'\n",
    "    \n",
    "    else:\n",
    "        return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_image_and_XML(file):\n",
    "\n",
    "    my_file = open(file, 'r')\n",
    "    string = my_file.read()\n",
    "\n",
    "    path_image = file.replace('annotations', 'images')\n",
    "    image = cv2.imread(path_image[:-3]+'jpg')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    bboxes = []\n",
    "    class_labels = []\n",
    "    \n",
    "    for item in string.split('</object>'):\n",
    "        if '<object>' in item:\n",
    "            bboxes.append([int(item[item.find('<xmin>')+len('<xmin>') : item.find('</xmin>')]),\n",
    "                           int(item[item.find('<ymin>')+len('<ymin>') : item.find('</ymin>')]),\n",
    "                           int(item[item.find('<xmax>')+len('<xmax>') : item.find('</xmax>')]),\n",
    "                           int(item[item.find('<ymax>')+len('<ymax>') : item.find('</ymax>')])])\n",
    "            class_labels.append(item[item.find('<name>')+len('<name>') : item.find('</name>')])\n",
    "\n",
    "    transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ChannelDropout(channel_drop_range=(1, 1), fill_value=0, p=0.1),\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', min_area=1024, min_visibility=0.1, label_fields=['class_labels']))\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "        transformed_image = transformed['image']\n",
    "        transformed_bboxes = transformed['bboxes']\n",
    "        transformed_class_labels = transformed['class_labels']\n",
    "\n",
    "        # сохранить изображение\n",
    "        Image.fromarray(transformed_image).save(f'{path_image[:-4]}-{i}.jpg',quality=95)\n",
    "        \n",
    "        # сохранить аннотацию \n",
    "        folder = string[string.find('<folder>')+len('<folder>') : string.find('</folder>')]\n",
    "        filename = string[string.find('<filename>')+len('<filename>') : string.find('</filename>')]\n",
    "        width = string[string.find('<width>')+len('<width>') : string.find('</width>')]\n",
    "        height = string[string.find('<height>')+len('<height>') : string.find('</height>')]\n",
    "        \n",
    "        annotation = ET.Element(\"annotation\")\n",
    "        ET.SubElement(annotation, \"folder\").text = folder\n",
    "        ET.SubElement(annotation, \"filename\").text = str(f'{filename[:-4]}-{i}.jpg')\n",
    "        ET.SubElement(annotation, \"path\").text = str(f'{path_image[:-4]}-{i}.jpg')\n",
    "        source = ET.SubElement(annotation, \"source\")\n",
    "        ET.SubElement(source, \"database\").text = \"Unknown\"\n",
    "        size = ET.SubElement(annotation, \"size\")\n",
    "        ET.SubElement(size, \"width\").text = str(width)\n",
    "        ET.SubElement(size, \"height\").text = str(height)\n",
    "        ET.SubElement(size, \"depth\").text = \"3\"\n",
    "        ET.SubElement(annotation, \"segmented\").text = \"0\"\n",
    "        for num, box in enumerate(transformed_bboxes):\n",
    "            object = ET.SubElement(annotation, \"object\")\n",
    "            ET.SubElement(object, \"name\").text = transformed_class_labels[num]\n",
    "            ET.SubElement(object, \"pose\").text = \"Unspecified\"\n",
    "            ET.SubElement(object, \"truncated\").text = \"0\"\n",
    "            ET.SubElement(object, \"difficult\").text = \"0\"\n",
    "            bndbox = ET.SubElement(object, \"bndbox\")\n",
    "            ET.SubElement(bndbox, \"xmin\").text = str(int(box[0])) \n",
    "            ET.SubElement(bndbox, \"ymin\").text = str(int(box[1]))\n",
    "            ET.SubElement(bndbox, \"xmax\").text = str(int(box[2]))\n",
    "            ET.SubElement(bndbox, \"ymax\").text = str(int(box[3]))\n",
    "\n",
    "        tree = ET.ElementTree(annotation)\n",
    "        tree.write(f'{file[:-4]}-{i}.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание директорий для сохранение предобработанных данных\n",
    "os.makedirs('imageai/data/train/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/train/annotations', exist_ok=True)\n",
    "\n",
    "os.makedirs('imageai/data/validation/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/validation/annotations', exist_ok=True)\n",
    "\n",
    "os.makedirs('imageai/data/test/images', exist_ok=True)\n",
    "os.makedirs('imageai/data/test/annotations', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple' 'orange' 'banana']\n"
     ]
    }
   ],
   "source": [
    "# формирование списка классов\n",
    "set_classes = set()\n",
    "annots_path = annots_train_path + annots_test_path\n",
    "\n",
    "for annot in annots_path:\n",
    "    with open(annot, 'r') as f:\n",
    "        string = f.read()\n",
    "        name = string[string.find('<name>')+6:string.find('</name>')]\n",
    "        set_classes.add(name)\n",
    "\n",
    "classes = np.array(list(set_classes))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (annot_path, img_path) in enumerate(zip(annots_train_path, images_train_path)):   \n",
    "    if i > n_imgs:\n",
    "        break\n",
    "        \n",
    "    if i%5 == 0:\n",
    "        new_path = shutil.copy(img_path, 'imageai/data/validation/images/' + img_path.parts[-1])\n",
    "        # редактирование валидационных данных\n",
    "        annot_path_edit = editXML(annot_path)\n",
    "        annot_path_new = shutil.copy(annot_path_edit, 'imageai/data/validation/annotations/' + annot_path.parts[-1])\n",
    "        # аугментация валидационных данных\n",
    "        augmentation_image_and_XML(annot_path_new)\n",
    "    else:\n",
    "        shutil.copy(img_path, 'imageai/data/train/images/' + img_path.parts[-1])\n",
    "        # редактирование тренировочных данных\n",
    "        annot_path_edit = editXML(annot_path)\n",
    "        annot_path_new = shutil.copy(annot_path_edit, 'imageai/data/train/annotations/' + annot_path.parts[-1])\n",
    "        # аугментация тренировочных данных\n",
    "        augmentation_image_and_XML(annot_path_new)\n",
    "        \n",
    "for i, (annot_test_path, img_test_path) in enumerate(zip(annots_test_path, images_test_path)):  \n",
    "    shutil.copy(img_test_path, 'imageai/data/test/images/' + img_test_path.parts[-1])\n",
    "    annot_test_path_new = editXML(annot_test_path)\n",
    "    shutil.copy(annot_test_path_new, 'imageai/data/test/annotations/' + annot_test_path.parts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m detector\u001b[38;5;241m.\u001b[39msetModelTypeAsYOLOv3()\n\u001b[0;32m      3\u001b[0m detector\u001b[38;5;241m.\u001b[39msetModelPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dz7/yolo.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m detections \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetectObjectsFromImage(input_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_dz7/train_zip/train/apple_31.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                              output_image_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetected.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                              minimum_percentage_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eachObject \u001b[38;5;129;01min\u001b[39;00m detections:\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\__init__.py:210\u001b[0m, in \u001b[0;36mObjectDetection.loadModel\u001b[1;34m(self, detection_speed)\u001b[0m\n\u001b[0;32m    207\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__yolo_input_image_shape \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mplaceholder(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,))\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__yolo_boxes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__yolo_scores, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__yolo_classes \u001b[38;5;241m=\u001b[39m \u001b[43myolo_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__yolo_anchors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumbers_to_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__yolo_input_image_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__yolo_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__yolo_iou\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__model_collection\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__modelLoaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\YOLOv3\\utils.py:86\u001b[0m, in \u001b[0;36myolo_eval\u001b[1;34m(yolo_outputs, anchors, num_classes, image_shape, max_boxes, score_threshold, iou_threshold)\u001b[0m\n\u001b[0;32m     84\u001b[0m box_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers):\n\u001b[1;32m---> 86\u001b[0m     _boxes, _box_scores \u001b[38;5;241m=\u001b[39m \u001b[43myolo_boxes_and_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m[\u001b[49m\u001b[43manchor_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     boxes\u001b[38;5;241m.\u001b[39mappend(_boxes)\n\u001b[0;32m     89\u001b[0m     box_scores\u001b[38;5;241m.\u001b[39mappend(_box_scores)\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\YOLOv3\\utils.py:63\u001b[0m, in \u001b[0;36myolo_boxes_and_scores\u001b[1;34m(feats, anchors, num_classes, input_shape, image_shape)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myolo_boxes_and_scores\u001b[39m(feats, anchors, num_classes, input_shape, image_shape):\n\u001b[1;32m---> 63\u001b[0m     box_xy, box_wh, box_confidence, box_class_probs \u001b[38;5;241m=\u001b[39m \u001b[43myolo_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n\u001b[0;32m     66\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mreshape(boxes, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m])\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\YOLOv3\\utils.py:25\u001b[0m, in \u001b[0;36myolo_head\u001b[1;34m(feats, anchors, num_classes, input_shape, calc_loss)\u001b[0m\n\u001b[0;32m     19\u001b[0m grid \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mcast(grid, K\u001b[38;5;241m.\u001b[39mdtype(feats))\n\u001b[0;32m     21\u001b[0m feats \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m     22\u001b[0m     feats, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, grid_shape[\u001b[38;5;241m0\u001b[39m], grid_shape[\u001b[38;5;241m1\u001b[39m], num_anchors, num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m---> 25\u001b[0m box_xy \u001b[38;5;241m=\u001b[39m (K\u001b[38;5;241m.\u001b[39msigmoid(feats[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m+\u001b[39m grid) \u001b[38;5;241m/\u001b[39m K\u001b[38;5;241m.\u001b[39mcast(\u001b[43mgrid_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, K\u001b[38;5;241m.\u001b[39mdtype(feats))\n\u001b[0;32m     26\u001b[0m box_wh \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(feats[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m]) \u001b[38;5;241m*\u001b[39m anchors_tensor \u001b[38;5;241m/\u001b[39m K\u001b[38;5;241m.\u001b[39mcast(input_shape[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], K\u001b[38;5;241m.\u001b[39mdtype(feats))\n\u001b[0;32m     27\u001b[0m box_confidence \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msigmoid(feats[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m5\u001b[39m])\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py:534\u001b[0m, in \u001b[0;36mTFSlicingOpDispatcher.handle\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(_slice_to_dict, kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(x, keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor)\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten([args, kwargs])):\n\u001b[1;32m--> 534\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SlicingOpLambda(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    536\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)"
     ]
    }
   ],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"data_dz7/yolo.h5\")\n",
    "detector.loadModel()\n",
    "detections = detector.detectObjectsFromImage(input_image='data_dz7/train_zip/train/apple_31.jpg',\n",
    "                                             output_image_path='detected.jpg',\n",
    "                                             minimum_percentage_probability=50)\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "    print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
